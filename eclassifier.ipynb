{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Emotion Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import requests\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import spacy\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# use the GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have downloaded before, if you want to overwrite them, please pass parameter: overwrite = True\n"
     ]
    }
   ],
   "source": [
    "# download datasets from github by using github api \n",
    "def download_datasets(overwrite = False):\n",
    "    api_url = f\"https://api.github.com/repos/cardiffnlp/tweeteval/contents/datasets/emotion\"\n",
    "\n",
    "    # if there is a not empty folder called ex3_datasets at present word dictionary, we think you have already downloaded datasets\n",
    "    # if you want to download anyway, please set the parameter overwrite as True\n",
    "    # When the overwrite is set as True, the old txt file will be overwrited.\n",
    "    if not os.path.exists(\"./ex3_datasets\"):\n",
    "        os.makedirs(\"./ex3_datasets\")\n",
    "        response = requests.get(api_url)\n",
    "    elif os.path.exists(\"./ex3_datasets\") and not os.listdir(\"./ex3_datasets\"):\n",
    "        response = requests.get(api_url)\n",
    "    elif not overwrite:\n",
    "        print(\"Files have downloaded before, if you want to overwrite them, please pass parameter: overwrite = True\")\n",
    "        return\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    " \n",
    "        for item in data:\n",
    "            file_url = item['download_url']\n",
    "            file_name = item['name']\n",
    "            response = requests.get(file_url)\n",
    "        \n",
    "            if response.status_code == 200:\n",
    "                with open('./ex3_datasets/' + file_name, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                print(f\"{file_name} is downloaded sucessfully!\")\n",
    "            else:\n",
    "                print(f\"{file_name}: failed!\")\n",
    "    else:\n",
    "        print(f\"Fail! HTTP respond code: {response.status_code}\")\n",
    "\n",
    "download_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "with open(f'./ex3_datasets/train_text.txt') as f:\n",
    "    x_train = f.read().splitlines()\n",
    "with open(f'./ex3_datasets/train_labels.txt') as f:\n",
    "    y_train = f.read().splitlines()\n",
    "with open(f'./ex3_datasets/val_text.txt') as f:\n",
    "    x_val = f.read().splitlines()\n",
    "with open(f'./ex3_datasets/val_labels.txt') as f:\n",
    "    y_val = f.read().splitlines()\n",
    "with open(f'./ex3_datasets/test_text.txt') as f:\n",
    "    x_test = f.read().splitlines()\n",
    "with open(f'./ex3_datasets/test_labels.txt') as f:\n",
    "    y_test = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        texts = []\n",
    "        for line in f:\n",
    "            texts.append(line.decode(errors='ignore').lower().strip())\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data set: data with labels 0 and 1 left\n",
    "def filter_data(arr, x, y):\n",
    "    index = [i for i, v in enumerate(y) if v in arr]\n",
    "    y = [int(v) for v in y if v in arr]\n",
    "    x = [v for i, v in enumerate(x) if i in index]\n",
    "    return x, y\n",
    "\n",
    "anger_joy_arr = ['0','1']\n",
    "anger_sadness_arr = ['0','3']\n",
    "\n",
    "# prepare anger and joy record\n",
    "x_train_1, y_train_1 = filter_data(anger_joy_arr, x_train, y_train)\n",
    "x_val_1, y_val_1 = filter_data(anger_joy_arr, x_val, y_val)\n",
    "x_test_1, y_test_1 = filter_data(anger_joy_arr, x_test, y_test)\n",
    "\n",
    "# prepare anger and sadness record\n",
    "x_train_2, y_train_2 = filter_data(anger_sadness_arr, x_train, y_train)\n",
    "x_val_2, y_val_2 = filter_data(anger_sadness_arr, x_val, y_val)\n",
    "x_test_2, y_test_2 = filter_data(anger_sadness_arr, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\10405\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\10405\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, use_stem=False):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stemming (you can choose to use stemming or lemmatization)\n",
    "    if use_stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "  \"\"\"\n",
    "  Assign unique id to each token\n",
    "  \"\"\"\n",
    "  max_lens = []\n",
    "  tokenized_texts = []\n",
    "  word2idx = {}\n",
    "\n",
    "  # Add <pad> and <unk> tokens to the vocabulary\n",
    "  word2idx['<pad>'] = 0\n",
    "  word2idx['<unk>'] = 1\n",
    "\n",
    "  # Building our vocab from the corpus starting from index 2\n",
    "  idx = 2\n",
    "  for text in texts:\n",
    "    tokenized_text = []\n",
    "    max_len = 0\n",
    "    for sent in text:\n",
    "      tokenized_sent = nlp(sent)\n",
    "      # Add `tokenized_sent` to `tokenized_texts`\n",
    "      tokenized_text.append(tokenized_sent)\n",
    "      # Add new token to `word2idx`\n",
    "      for token in tokenized_sent:\n",
    "        # string any token objects are different things, be careful.\n",
    "        if token.text not in word2idx:\n",
    "          word2idx[token.text] = idx\n",
    "          idx += 1\n",
    "\n",
    "          # Update `max_len`\n",
    "      max_len = max(max_len, len(tokenized_sent))\n",
    "\n",
    "    tokenized_texts.append(tokenized_text)\n",
    "    max_lens.append(max_len)\n",
    "\n",
    "  return tokenized_texts, word2idx, max_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokenized_texts, word2idx, max_len):\n",
    "    input_ids = []\n",
    "    for i, tokenized_text in enumerate(tokenized_texts):\n",
    "        input_ids_temp = []\n",
    "        for tokenized_sent in tokenized_text:\n",
    "            # Pad sentences to max_len\n",
    "            tokenized_padded_sent = list(tokenized_sent) + ['<pad>'] * (max_len[i] - len(tokenized_sent))\n",
    "\n",
    "            # Encode tokens to input_ids\n",
    "            input_id = [word2idx.get(str(token)) for token in tokenized_padded_sent]\n",
    "            input_ids_temp.append(input_id)\n",
    "        \n",
    "        input_ids.append(np.array(input_ids_temp, dtype=np.int64))\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts, word2idx, max_lens = tokenize([x_train_1,x_val_1, x_test_1])\n",
    "input_ids_1 = encode(tokenized_texts, word2idx, max_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data type to torch.Tensor\n",
    "train_inputs_1 = torch.from_numpy(input_ids_1[0])\n",
    "train_labels_1 = torch.tensor(y_train_1, dtype=torch.long)\n",
    "\n",
    "val_input_1 = torch.from_numpy(input_ids_1[1])\n",
    "val_labels_1 = torch.tensor(y_val_1, dtype=torch.long)\n",
    "\n",
    "test_input_1 = torch.from_numpy(input_ids_1[2])\n",
    "test_labels_1 = torch.tensor(y_test_1, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = TensorDataset(train_inputs_1, train_labels_1)\n",
    "val_data_1 = TensorDataset(val_input_1, val_labels_1)\n",
    "test_data_1 = TensorDataset(test_input_1, test_labels_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(train_data_1, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_data_1)\n",
    "test_dataloader = DataLoader(test_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=len(word2idx),\n",
    "                 embed_dim=300,\n",
    "                 filter_sizes=[3, 4, 5],\n",
    "                 num_filters=[100, 100, 100],\n",
    "                 num_classes=2,\n",
    "                 dropout=0.5):\n",
    "        \"\"\"\n",
    "        The constructor for CNN class.\n",
    "        Args:\n",
    "            vocab_size (int): Need to be specified when pretrained word\n",
    "                embeddings are not used.\n",
    "            embed_dim (int): Dimension of word vectors. Need to be specified\n",
    "                when pretrained word embeddings are not used. Default: 300\n",
    "            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n",
    "            num_filters (List[int]): List of number of filters, has the same\n",
    "                length as `filter_sizes`. Default: [100, 100, 100]\n",
    "            n_classes (int): Number of classes. Default: 2\n",
    "            dropout (float): Dropout rate. Default: 0.5\n",
    "        \"\"\"\n",
    "\n",
    "        super(CNN, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                          embedding_dim=self.embed_dim,\n",
    "                                          padding_idx=0,\n",
    "                                          max_norm=5.0)\n",
    "        # Conv Network\n",
    "        self.conv1d_list = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=self.embed_dim,\n",
    "                      out_channels=num_filters[i],\n",
    "                      kernel_size=filter_sizes[i])\n",
    "            for i in range(len(filter_sizes))\n",
    "        ])\n",
    "        # Fully-connected layer and Dropout\n",
    "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): A tensor of token ids with shape\n",
    "                (batch_size, max_sent_length)\n",
    "\n",
    "        Returns:\n",
    "            logits (torch.Tensor): Output logits with shape (batch_size,\n",
    "                n_classes)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get embeddings from `input_ids`. Output shape: (batch_size, max_len, embed_dim)\n",
    "        x_embed = self.embedding(input_ids).float()\n",
    "\n",
    "        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n",
    "        # Output shape: (b, embed_dim, max_len)\n",
    "        x_reshaped = x_embed.permute(0, 2, 1)\n",
    "\n",
    "        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n",
    "        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n",
    "\n",
    "        # Max pooling. Output shape: (b, num_filters[i], 1)\n",
    "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
    "            for x_conv in x_conv_list]\n",
    "\n",
    "        # Concatenate x_pool_list to feed the fully connected layer.\n",
    "        # Output shape: (b, sum(num_filters))\n",
    "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n",
    "                         dim=1)\n",
    "        # print(x_fc.shape)\n",
    "\n",
    "        # Compute logits. Output shape: (b, n_classes)\n",
    "        logits = self.fc(self.dropout(x_fc))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CNN model\n",
    "cnn1_1 = CNN(embed_dim=300,\n",
    "            filter_sizes=[3, 4, 5],\n",
    "            num_filters=[100, 100, 100],\n",
    "            num_classes=2,\n",
    "            dropout=0.5)\n",
    "\n",
    "# Send model to `device` (GPU/CPU)\n",
    "cnn1_1.to(device)\n",
    "\n",
    "# Instantiate Adadelta optimizer\n",
    "optimizer_1 = optim.Adadelta(cnn1_1.parameters(),\n",
    "                               lr=0.0001,\n",
    "                               rho=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataloader, val_dataloader): \n",
    "  # Specify loss function\n",
    "  loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "  # Start training loop\n",
    "  print(\"Start training...\\n\")\n",
    "  print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^12}\")\n",
    "  print(\"-\"*60)\n",
    "\n",
    "  for epoch_i in range(10):\n",
    "    total_loss = 0\n",
    "    # Put the model into the training mode\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "      # Load batch to GPU\n",
    "      b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "      # Zero out any previously calculated gradients\n",
    "      model.zero_grad()\n",
    "\n",
    "      # Perform a forward pass. This will return logits.\n",
    "      logits = model(b_input_ids)\n",
    "      #break\n",
    "\n",
    "      # Compute loss and accumulate the loss values\n",
    "      loss = loss_fn(logits, b_labels)\n",
    "\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      # Perform a backward pass to calculate gradients\n",
    "      loss.backward()\n",
    "\n",
    "      # Update parameters\n",
    "      optimizer.step()\n",
    "\n",
    "      # Calculate the average loss over the entire training data\n",
    "      avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "\n",
    "      total_val_loss = 0\n",
    "\n",
    "      for step, batch in enumerate(val_dataloader):\n",
    "\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        logits = model(b_input_ids)\n",
    "\n",
    "        val_loss = loss_fn(logits, b_labels)\n",
    "\n",
    "        total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "    print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {avg_val_loss:^12.6f} \\n\")\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |   Val Loss  \n",
      "------------------------------------------------------------\n",
      "   1    |   5.354104   |   6.015400   \n",
      "\n",
      "   2    |   5.310530   |   6.006713   \n",
      "\n",
      "   3    |   5.331689   |   6.030020   \n",
      "\n",
      "   4    |   5.297817   |   5.981511   \n",
      "\n",
      "   5    |   5.262202   |   6.023051   \n",
      "\n",
      "   6    |   5.215111   |   5.963070   \n",
      "\n",
      "   7    |   5.241347   |   6.048886   \n",
      "\n",
      "   8    |   5.228076   |   5.882992   \n",
      "\n",
      "   9    |   5.228333   |   5.891965   \n",
      "\n",
      "  10    |   5.173050   |   5.780343   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn1_1 = train(cnn1_1, optimizer_1, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, labels, dataloader):\n",
    "\n",
    "    predicted_labels = []\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "            predicted = torch.argmax(logits, dim=1).cpu().flatten().numpy()\n",
    "\n",
    "            predicted_labels.extend(predicted.tolist())\n",
    "\n",
    "            correct += np.sum((predicted==(b_labels.cpu().numpy())).astype(int))\n",
    "\n",
    "        _, _, f1, _ = precision_recall_fscore_support(labels.tolist(), predicted_labels, average='macro')\n",
    "\n",
    "        acc = correct / len(labels)\n",
    "    \n",
    "    print(acc)\n",
    "    print(f1)\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5786026200873362\n",
      "0.4536936892393837\n"
     ]
    }
   ],
   "source": [
    "acc1_1, f1_1_1 = model_eval(cnn1_1, test_labels_1, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CNN model\n",
    "cnn1_2 = CNN(embed_dim=300,\n",
    "            filter_sizes=[3, 4, 5],\n",
    "            num_filters=[100, 100, 100],\n",
    "            num_classes=2,\n",
    "            dropout=0.5)\n",
    "\n",
    "# Send model to `device` (GPU/CPU)\n",
    "cnn1_2.to(device)\n",
    "\n",
    "# Instantiate Adadelta optimizer\n",
    "optimizer_2 = optim.Adam(cnn1_2.parameters(),\n",
    "                               lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |   Val Loss  \n",
      "------------------------------------------------------------\n",
      "   1    |   0.717986   |   0.693579   \n",
      "\n",
      "   2    |   0.665863   |   0.680757   \n",
      "\n",
      "   3    |   0.635314   |   0.669511   \n",
      "\n",
      "   4    |   0.626067   |   0.670366   \n",
      "\n",
      "   5    |   0.616355   |   0.679254   \n",
      "\n",
      "   6    |   0.605228   |   0.679100   \n",
      "\n",
      "   7    |   0.596914   |   0.677550   \n",
      "\n",
      "   8    |   0.597610   |   0.681434   \n",
      "\n",
      "   9    |   0.588854   |   0.674347   \n",
      "\n",
      "  10    |   0.587875   |   0.660362   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn1_2 = train(cnn1_2, optimizer_2, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.611353711790393\n",
      "0.3871988574435299\n"
     ]
    }
   ],
   "source": [
    "acc1_2, f1_1_2 = model_eval(cnn1_2, test_labels_1, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CNN model\n",
    "cnn1_3 = CNN(embed_dim=300,\n",
    "            filter_sizes=[3, 5, 7],\n",
    "            num_filters=[100, 100, 100],\n",
    "            num_classes=2,\n",
    "            dropout=0.3)\n",
    "\n",
    "# Send model to `device` (GPU/CPU)\n",
    "cnn1_3.to(device)\n",
    "\n",
    "# Instantiate Adadelta optimizer\n",
    "optimizer_3 = optim.AdamW(cnn1_3.parameters(),\n",
    "                               lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  |   Val Loss  \n",
      "------------------------------------------------------------\n",
      "   1    |   0.666585   |   0.669307   \n",
      "\n",
      "   2    |   0.633026   |   0.661011   \n",
      "\n",
      "   3    |   0.615734   |   0.661195   \n",
      "\n",
      "   4    |   0.606105   |   0.648152   \n",
      "\n",
      "   5    |   0.596460   |   0.655633   \n",
      "\n",
      "   6    |   0.590363   |   0.657398   \n",
      "\n",
      "   7    |   0.582103   |   0.666376   \n",
      "\n",
      "   8    |   0.570982   |   0.653069   \n",
      "\n",
      "   9    |   0.564006   |   0.646005   \n",
      "\n",
      "  10    |   0.554060   |   0.655791   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn1_3 = train(cnn1_3, optimizer_3, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6146288209606987\n",
      "0.3961247910655424\n"
     ]
    }
   ],
   "source": [
    "acc1_3, f1_1_3 = model_eval(cnn1_3, test_labels_1, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
